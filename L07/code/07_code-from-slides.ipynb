{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAT451: Machine Learning -- L07: Ensemble Methods (Code used in the slides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAT 451: Intro to Machine Learning (Fall 2020)  \n",
    "Instructor: Sebastian Raschka (sraschka@wisc.edu)  \n",
    "\n",
    "Course website: http://stat.wisc.edu/~sraschka/teaching/stat451-fs2020/  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Valid/Test sizes: 84 28 38\n",
      "Validation Accuracy: 0.86 [Classifier 1]\n",
      "Validation Accuracy: 0.82 [Classifier 2]\n",
      "Validation Accuracy: 0.93 [Classifier 3]\n",
      "Validation Accuracy: 0.93 [Ensemble]\n",
      "Test Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, [0, 3]], iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = \\\n",
    "    train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "\n",
    "print('Train/Valid/Test sizes:', y_train.shape[0], y_val.shape[0], y_test.shape[0])\n",
    "\n",
    "clf1 = DecisionTreeClassifier(random_state=1)\n",
    "clf2 = DecisionTreeClassifier(random_state=1, max_depth=1)\n",
    "clf3 = DecisionTreeClassifier(random_state=1, max_depth=3)\n",
    "eclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3], weights=[1, 1, 1])\n",
    "\n",
    "labels = ['Classifier 1', 'Classifier 2', 'Classifier 3', 'Ensemble']\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], labels):\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(\"Validation Accuracy: %0.2f [%s]\" % (clf.score(X_val, y_val), label))\n",
    "    \n",
    "print(\"Test Accuracy: %0.2f\" % eclf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Valid/Test sizes: 84 28 38\n",
      "OOB Accuracy: 0.93\n",
      "Test Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, [0, 3]], iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = \\\n",
    "    train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "\n",
    "print('Train/Valid/Test sizes:', y_train.shape[0], y_val.shape[0], y_test.shape[0])\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='entropy',\n",
    "                              random_state=1,\n",
    "                              max_depth=None)\n",
    "\n",
    "\n",
    "bag = BaggingClassifier(base_estimator=tree,\n",
    "                        n_estimators=500,\n",
    "                        oob_score=True,\n",
    "                        bootstrap=True,\n",
    "                        bootstrap_features=False,\n",
    "                        n_jobs=1,\n",
    "                        random_state=1)\n",
    "\n",
    "bag.fit(X_train, y_train)\n",
    "    \n",
    "print(\"OOB Accuracy: %0.2f\" % bag.oob_score_)\n",
    "print(\"Test Accuracy: %0.2f\" % bag.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(criterion='entropy',\n",
    "                              random_state=1,\n",
    "                              max_depth=None)\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Test Accuracy: %0.2f\" % tree.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.58\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier(criterion='entropy',\n",
    "                              random_state=1,\n",
    "                              max_depth=1)\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Test Accuracy: %0.2f\" % tree.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Valid/Test sizes: 84 28 38\n",
      "Test Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, [0, 3]], iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = \\\n",
    "    train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "\n",
    "print('Train/Valid/Test sizes:', y_train.shape[0], y_val.shape[0], y_test.shape[0])\n",
    "\n",
    "tree = DecisionTreeClassifier(criterion='entropy',\n",
    "                              random_state=1,\n",
    "                              max_depth=1)\n",
    "\n",
    "\n",
    "boost = AdaBoostClassifier(base_estimator=tree,\n",
    "                           n_estimators=500,\n",
    "                           algorithm='SAMME',\n",
    "                           #n_jobs=1,\n",
    "                           random_state=1)\n",
    "\n",
    "boost.fit(X_train, y_train)\n",
    "    \n",
    "print(\"Test Accuracy: %0.2f\" % boost.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boost.estimator_weights_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Valid/Test sizes: 84 28 38\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, [0, 3]], iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = \\\n",
    "    train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "\n",
    "print('Train/Valid/Test sizes:', y_train.shape[0], y_val.shape[0], y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "boost = GradientBoostingClassifier(\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    max_depth=8,\n",
    "    random_state=1)\n",
    "\n",
    "boost.fit(X_train, y_train)\n",
    "    \n",
    "print(\"Test Accuracy: %0.2f\" % boost.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "\n",
    "boost = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.1,\n",
    "    #n_estimators=100,\n",
    "    max_depth=8,\n",
    "    random_state=1)\n",
    "\n",
    "boost.fit(X_train, y_train)\n",
    "    \n",
    "print(\"Test Accuracy: %0.2f\" % boost.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "\n",
    "boost = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.1,\n",
    "    max_iter=100,\n",
    "    early_stopping=True,\n",
    "    max_depth=8,\n",
    "    random_state=1)\n",
    "\n",
    "boost.fit(X_train, y_train)\n",
    "    \n",
    "print(\"Test Accuracy: %0.2f\" % boost.score(X_test, y_test))\n",
    "\n",
    "boost.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.95\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "\n",
    "boost = HistGradientBoostingClassifier(\n",
    "    learning_rate=0.5,\n",
    "    max_iter=100,\n",
    "    early_stopping=True,\n",
    "    max_depth=8,\n",
    "    random_state=1)\n",
    "\n",
    "boost.fit(X_train, y_train)\n",
    "    \n",
    "print(\"Test Accuracy: %0.2f\" % boost.score(X_test, y_test))\n",
    "\n",
    "boost.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://xgboost.readthedocs.io/en/latest/build.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "\n",
    "param = {\n",
    "    'max_depth': 8,\n",
    "    'eta': 0.1,  # learning rate\n",
    "    'objective': 'multi:softprob',  # loss function for multiclass\n",
    "    'num_class': 3}  # number of classes\n",
    "\n",
    "boost = xgb.train(param, dtrain, num_boost_round=100)\n",
    "\n",
    "y_pred = boost.predict(dtest)\n",
    "y_labels = np.argmax(y_pred, axis=1)\n",
    "\n",
    "\n",
    "print(\"Test Accuracy: %0.2f\" % (y_labels == y_test).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sebastian/miniconda3/lib/python3.8/site-packages/lightgbm/__init__.py:42: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  warnings.warn(\"Starting from version 2.2.1, the library file in distribution wheels for macOS \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "# https://lightgbm.readthedocs.io/en/latest/Installation-Guide.html\n",
    "# conda install -c conda-forge lightgbm\n",
    "\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "boost = lgb.LGBMClassifier(n_estimators=100,\n",
    "                           max_depth=8,\n",
    "                           random_state=1,\n",
    "                           learning_rate=0.1)\n",
    "\n",
    "boost.fit(X_train, y_train)\n",
    "\n",
    "print(\"Test Accuracy: %0.2f\" % boost.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Valid/Test sizes: 84 28 38\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, [0, 3]], iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = \\\n",
    "    train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "\n",
    "print('Train/Valid/Test sizes:', y_train.shape[0], y_val.shape[0], y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100,\n",
    "                                random_state=1)\n",
    "\n",
    "forest.fit(X_train, y_train)\n",
    "    \n",
    "print(\"Test Accuracy: %0.2f\" % forest.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "\n",
    "forest = ExtraTreesClassifier(n_estimators=100,\n",
    "                              random_state=1)\n",
    "\n",
    "forest.fit(X_train, y_train)\n",
    "    \n",
    "print(\"Test Accuracy: %0.2f\" % forest.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from mlxtend.classifier import StackingClassifier\n",
    "\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=5)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = HistGradientBoostingClassifier(random_state=1)\n",
    "clf4 = AdaBoostClassifier(random_state=1)\n",
    "clf5 = DecisionTreeClassifier(random_state=1,\n",
    "                              max_depth=None)\n",
    "\n",
    "lr = LogisticRegression(random_state=1)\n",
    "\n",
    "sclf = StackingClassifier(classifiers=[clf1, clf2, clf3, clf4, clf5], \n",
    "                          meta_classifier=lr)\n",
    "\n",
    "\n",
    "sclf.fit(X_train, y_train)\n",
    "print(\"Train Accuracy: %0.2f\" % sclf.score(X_train, y_train))\n",
    "print(\"Test Accuracy: %0.2f\" % sclf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=5)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = HistGradientBoostingClassifier(random_state=1)\n",
    "clf4 = AdaBoostClassifier(random_state=1)\n",
    "clf5 = DecisionTreeClassifier(random_state=1,\n",
    "                              max_depth=None)\n",
    "\n",
    "lr = LogisticRegression(random_state=1)\n",
    "\n",
    "sclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3, clf4, clf5], \n",
    "                            meta_classifier=lr, \n",
    "                            cv=10,\n",
    "                            random_state=1)\n",
    "\n",
    "\n",
    "sclf.fit(X_train, y_train)\n",
    "print(\"Train Accuracy: %0.2f\" % sclf.score(X_train, y_train))\n",
    "print(\"Test Accuracy: %0.2f\" % sclf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "\n",
    "clf1 = KNeighborsClassifier(n_neighbors=5)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = HistGradientBoostingClassifier(random_state=1)\n",
    "clf4 = AdaBoostClassifier(random_state=1)\n",
    "clf5 = DecisionTreeClassifier(random_state=1,\n",
    "                              max_depth=None)\n",
    "\n",
    "lr = LogisticRegression(random_state=1)\n",
    "\n",
    "estimators = [('clf1', clf1),\n",
    "              ('clf2', clf2),\n",
    "              ('clf3', clf3),\n",
    "              ('clf4', clf4),\n",
    "              ('clf5', clf5)]\n",
    "\n",
    "sclf = StackingClassifier(estimators=estimators, \n",
    "                          final_estimator=lr, \n",
    "                          cv=10)\n",
    "\n",
    "\n",
    "sclf.fit(X_train, y_train)\n",
    "print(\"Train Accuracy: %0.2f\" % sclf.score(X_train, y_train))\n",
    "print(\"Test Accuracy: %0.2f\" % sclf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack_method{‘auto’, ‘predict_proba’, ‘decision_function’, ‘predict’}, default=’auto’\n",
    "\n",
    "\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "\n",
    "\n",
    "sclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3, clf4, clf5], \n",
    "                            meta_classifier=lr, \n",
    "                            use_probas=True,\n",
    "                            drop_proba_col='last',\n",
    "                            #use_features_in_secondary=True,\n",
    "                            cv=10,\n",
    "                            random_state=1)\n",
    "\n",
    "\n",
    "sclf.fit(X_train, y_train)\n",
    "print(\"Train Accuracy: %0.2f\" % sclf.score(X_train, y_train))\n",
    "print(\"Test Accuracy: %0.2f\" % sclf.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

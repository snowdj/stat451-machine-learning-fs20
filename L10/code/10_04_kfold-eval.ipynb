{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STAT 451: Machine Learning (Fall 2020)  \n",
    "Instructor: Sebastian Raschka (sraschka@wisc.edu)  \n",
    "\n",
    "Course website: http://pages.stat.wisc.edu/~sraschka/teaching/stat451-fs2020/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L10: Model Evaluation 3 -- Cross-Validation and Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Sebastian Raschka' -u -d -v -p numpy,mlxtend,matplotlib,sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "## K-fold Cross-Validation in Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simple demonstration of using a cross-validation iterator in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "\n",
    "y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
    "X = rng.random_sample((y.shape[0], 4))\n",
    "\n",
    "\n",
    "cv = KFold(n_splits=5)\n",
    "\n",
    "for k in cv.split(X, y):\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "- In practice, we are usually interested in shuffling the dataset, because if the data records are ordered by class label, this would result in cases where the classes are not well represented in the training and test folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, random_state=123, shuffle=True)\n",
    "\n",
    "for k in cv.split(X, y):\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "- Note that the `KFold` iterator only provides us with the array indices; in practice, we are actually interested in the array values (feature values and class labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5, random_state=123, shuffle=True)\n",
    "\n",
    "for train_idx, valid_idx in cv.split(X, y):\n",
    "    print('train labels with shuffling', y[train_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "- As discussed in the lecture, it's important to stratify the splits (very crucial for small datasets!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, random_state=123, shuffle=True)\n",
    "\n",
    "for train_idx, valid_idx in cv.split(X, y):\n",
    "    print('train labels', y[train_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "- After the illustrations of cross-validation above, the next cell demonstrates how we can actually use the iterators provided through scikit-learn to fit and evaluate a learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from mlxtend.data import iris_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X, y = iris_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123, test_size=0.15, \n",
    "                                                    shuffle=True, stratify=y)\n",
    "\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, random_state=123, shuffle=True)\n",
    "\n",
    "kfold_acc = 0.\n",
    "for train_idx, valid_idx in cv.split(X_train, y_train):\n",
    "    clf = DecisionTreeClassifier(random_state=123, max_depth=3).fit(X_train[train_idx], y_train[train_idx])\n",
    "    y_pred = clf.predict(X_train[valid_idx])\n",
    "    acc = np.mean(y_pred == y_train[valid_idx])*100\n",
    "    kfold_acc += acc\n",
    "kfold_acc /= 10\n",
    "    \n",
    "clf = DecisionTreeClassifier(random_state=123, max_depth=3).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "test_acc = np.mean(y_pred == y_test)*100\n",
    "    \n",
    "print('Kfold Accuracy: %.2f%%' % kfold_acc)\n",
    "print('Test Accuracy: %.2f%%' % test_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "- Usually, a more convenient way to use cross-validation through scikit-learn is to use the `cross_val_score` function (note that it performs stratifies splitting for classification by default)\n",
    "- (remember to ask students about whitespaces according to pep8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "cv_acc = cross_val_score(estimator=DecisionTreeClassifier(random_state=123, max_depth=3),\n",
    "                         X=X_train,\n",
    "                         y=y_train,\n",
    "                         cv=10,\n",
    "                         n_jobs=-1)\n",
    "\n",
    "print('Kfold Accuracy: %.2f%%' % (np.mean(cv_acc)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "- `cross_val_score` has unfortunately no way to specify a random seed; this is not an issue in regular use cases, but it is not useful if you want to do \"repeated cross-validation\"\n",
    "- The next cell illustrates how we can provide our own cross-validation iterator for convenience (note that the results match or \"manual\" `StratifiedKFold` approach we performed earlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "cv_acc = cross_val_score(estimator=DecisionTreeClassifier(random_state=123, max_depth=3),\n",
    "                         X=X_train,\n",
    "                         y=y_train,\n",
    "                         cv=StratifiedKFold(n_splits=10, random_state=123, shuffle=True),\n",
    "                         n_jobs=-1)\n",
    "\n",
    "print('Kfold Accuracy: %.2f%%' % (np.mean(cv_acc)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "##  Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall Bootstrapping from 2 lectures ago? Here I is an iterator I implemented analogous to `KFold`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import BootstrapOutOfBag\n",
    "\n",
    "oob = BootstrapOutOfBag(n_splits=5, random_seed=99)\n",
    "for train, test in oob.split(np.array([1, 2, 3, 4, 5])):\n",
    "    print(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "- Analagous the `KFold` iterator, we can use it in the `cross_val_score` function for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_acc = cross_val_score(estimator=DecisionTreeClassifier(random_state=99, max_depth=3),\n",
    "                         X=X_train,\n",
    "                         y=y_train,\n",
    "                         cv=BootstrapOutOfBag(n_splits=200, random_seed=99),\n",
    "                         n_jobs=-1)\n",
    "\n",
    "print('OOB Bootstrap Accuracy: %.2f%%' % (np.mean(cv_acc)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "- Analagous to the `cross_val_score` method, you can use the `bootstrap_point632_score`, which implements the .632-Bootstrap method (which is less pesimistically biased than the out-of-bag bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.evaluate import bootstrap_point632_score\n",
    "\n",
    "\n",
    "cv_acc = bootstrap_point632_score(estimator=DecisionTreeClassifier(random_state=123, max_depth=3),\n",
    "                                  X=X_train,\n",
    "                                  y=y_train,\n",
    "                                  random_seed=99)\n",
    "\n",
    "print('OOB Bootstrap Accuracy: %.2f%%' % (np.mean(cv_acc)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By default, `bootstrap_point632_score` uses the setting `method='.632'`\n",
    "- By setting `method='.632+'`, we can also perform the .632+ bootstrap, which corrects for optimism bias, which is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_acc = bootstrap_point632_score(estimator=DecisionTreeClassifier(random_state=123, max_depth=3),\n",
    "                                  X=X_train,\n",
    "                                  y=y_train,\n",
    "                                  method='.632+',\n",
    "                                  n_splits=200,\n",
    "                                  random_seed=99)\n",
    "\n",
    "print('OOB Bootstrap Accuracy: %.2f%%' % (np.mean(cv_acc)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, for your convenience, you can also set `method='oob'`, to run a regular Out-of-bag boostrap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_acc = bootstrap_point632_score(estimator=DecisionTreeClassifier(random_state=123, max_depth=3),\n",
    "                                  X=X_train,\n",
    "                                  y=y_train,\n",
    "                                  method='oob',\n",
    "                                  n_splits=200,\n",
    "                                  random_seed=99)\n",
    "\n",
    "print('OOB Bootstrap Accuracy: %.2f%%' % (np.mean(cv_acc)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
